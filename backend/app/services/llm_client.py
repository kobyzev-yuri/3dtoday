"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è LLM (Ollama –∏ ProxyAPI/OpenAI)
"""

import os
import logging
import httpx
from typing import Optional, List, Dict, Any
from pathlib import Path
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
load_dotenv(dotenv_path=Path(__file__).resolve().parents[3] / "config.env")

logger = logging.getLogger(__name__)


class LLMClient:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM (Ollama, OpenAI –∏–ª–∏ Gemini)
    """
    
    def __init__(self, provider: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        Args:
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM (openai, ollama, gemini). –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ config.env
        """
        self.provider = (provider or os.getenv("LLM_PROVIDER", "ollama")).lower()
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback"""
        providers_to_try = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –ø–æ–ø—ã—Ç–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        if self.provider == "ollama":
            providers_to_try = ["ollama", "gemini", "openai"]
        elif self.provider == "gemini":
            providers_to_try = ["gemini", "openai", "ollama"]
        elif self.provider == "openai":
            providers_to_try = ["openai", "gemini", "ollama"]
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–æ–±—É–µ–º –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
            providers_to_try = ["gemini", "openai", "ollama"]
        
        last_error = None
        for provider in providers_to_try:
            try:
                if provider == "ollama":
                    self._init_ollama()
                elif provider == "openai":
                    self._init_openai()
                elif provider == "gemini":
                    self._init_gemini()
                else:
                    continue
                
                # –ï—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞, –æ–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä
                if provider != self.provider:
                    logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä {provider} (–≤–º–µ—Å—Ç–æ {self.provider})")
                    self.provider = provider
                return
            except Exception as e:
                last_error = e
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å {provider}: {e}")
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
        error_msg = f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–∏–Ω LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}"
        logger.error(f"‚ùå {error_msg}")
        raise RuntimeError(error_msg)
    
    def _get_available_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama"""
        try:
            ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            response = httpx.get(f"{ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                return [model["name"] for model in data.get("models", [])]
            return []
        except Exception:
            return []
    
    def _init_ollama(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞"""
        try:
            self.ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            configured_model = os.getenv("OLLAMA_MODEL", "qwen3:8b")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –Ω–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é qwen –∏–ª–∏ –ø–µ—Ä–≤—É—é –≤ —Å–ø–∏—Å–∫–µ
            try:
                available_models = self._get_available_models()
                if available_models:
                    # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º qwen –º–æ–¥–µ–ª–∏
                    qwen_models = [m for m in available_models if 'qwen' in m.lower()]
                    preferred_models = qwen_models if qwen_models else available_models
                    
                    if configured_model not in available_models:
                        fallback_model = preferred_models[0] if preferred_models else available_models[0]
                        logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å '{configured_model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ–º '{fallback_model}'")
                        self.model = fallback_model
                    else:
                        self.model = configured_model
                        logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å Ollama: {self.model}")
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π. –ò—Å–ø–æ–ª—å–∑—É–µ–º '{configured_model}'")
                    self.model = configured_model
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º '{configured_model}'")
                self.model = configured_model
            
            self.temperature = float(os.getenv("OLLAMA_TEMPERATURE", "0.2"))
            self.timeout = int(os.getenv("OLLAMA_TIMEOUT", "500"))
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –µ—Å–ª–∏ –µ—Å—Ç—å fallback)
            if not self._check_ollama_available():
                raise ConnectionError(f"Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É {self.ollama_url}")
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç –±–µ–∑ —Ç–∞–π–º–∞—É—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, 
            # —Ç–∞–π–º–∞—É—Ç –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å—Å—è –≤ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å
            self.client = httpx.AsyncClient(
                base_url=self.ollama_url,
                timeout=None  # –¢–∞–π–º–∞—É—Ç –±—É–¥–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å—Å—è –≤ –∫–∞–∂–¥–æ–º –∑–∞–ø—Ä–æ—Å–µ
            )
            
            logger.info(f"‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (model={self.model}, url={self.ollama_url})")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Ollama: {e}")
            raise
    
    def _init_openai(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI/ProxyAPI –∫–ª–∏–µ–Ω—Ç–∞"""
        try:
            from openai import OpenAI
            
            api_key = os.getenv("OPENAI_API_KEY")
            base_url = os.getenv("OPENAI_BASE_URL", "https://api.proxyapi.ru/openai/v1")
            self.model = os.getenv("OPENAI_MODEL", "gpt-4o")
            self.temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.2"))
            self.timeout = int(os.getenv("OPENAI_TIMEOUT", "600"))
            
            if not api_key:
                raise ValueError("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ config.env")
            
            self.client = OpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=self.timeout
            )
            
            logger.info(f"‚úÖ OpenAI/ProxyAPI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (model={self.model})")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {e}")
            raise
    
    def _init_gemini(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini/ProxyAPI –∫–ª–∏–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ REST API"""
        try:
            api_key = os.getenv("GEMINI_API_KEY")
            base_url = os.getenv("GEMINI_BASE_URL", "https://api.proxyapi.ru/google")
            self.model = os.getenv("GEMINI_MODEL", "gemini-3-pro-preview")
            self.temperature = float(os.getenv("GEMINI_TEMPERATURE", "0.2"))
            self.timeout = int(os.getenv("GEMINI_TIMEOUT", "120"))
            
            if not api_key:
                raise ValueError("GEMINI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ config.env")
            
            # ProxyAPI.ru –∏—Å–ø–æ–ª—å–∑—É–µ—Ç REST API –Ω–∞–ø—Ä—è–º—É—é
            # –§–æ—Ä–º–∞—Ç: https://api.proxyapi.ru/google/v1beta/models/{model}:generateContent
            self.api_key = api_key
            self.base_url = base_url.rstrip('/')
            
            # –°–æ–∑–¥–∞–µ–º HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è ProxyAPI
            # –¢–∞–π–º–∞—É—Ç –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å—Å—è –≤ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å
            self.client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=None,  # –¢–∞–π–º–∞—É—Ç –±—É–¥–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å—Å—è –≤ –∫–∞–∂–¥–æ–º –∑–∞–ø—Ä–æ—Å–µ
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
            )
            
            logger.info(f"‚úÖ Gemini/ProxyAPI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (model={self.model}, base_url={self.base_url})")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini: {e}")
            raise
    
    def _check_ollama_available(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            response = httpx.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                logger.info("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (status={response.status_code})")
                return False
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            logger.warning("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama: ollama serve")
            return False
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM
        
        Args:
            prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if self.provider == "ollama":
            return await self._generate_ollama(prompt, system_prompt, temperature, max_tokens, timeout)
        elif self.provider == "openai":
            return await self._generate_openai(prompt, system_prompt, temperature, max_tokens, timeout)
        elif self.provider == "gemini":
            return await self._generate_gemini(prompt, system_prompt, temperature, max_tokens, timeout)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.provider}")
    
    async def _generate_ollama(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        request_timeout = timeout if timeout is not None else self.timeout
        try:
            # –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ /api/chat (–Ω–æ–≤—ã–π API)
            try:
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                payload = {
                    "model": self.model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature or self.temperature
                    }
                }
                
                if max_tokens:
                    payload["options"]["num_predict"] = max_tokens
                
                logger.debug(f"üì§ Ollama –∑–∞–ø—Ä–æ—Å –∫ /api/chat: model={self.model}, timeout={request_timeout}s")
                response = await self.client.post("/api/chat", json=payload, timeout=request_timeout)
                response.raise_for_status()
                
                result = response.json()
                content = result.get("message", {}).get("content", "")
                if content:
                    logger.debug(f"‚úÖ Ollama –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return content
                    
            except httpx.HTTPStatusError as e:
                if e.response.status_code == 404:
                    logger.warning(f"‚ö†Ô∏è /api/chat –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –ø—Ä–æ–±—É–µ–º /api/generate")
                    # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π API /api/generate
                else:
                    raise
            
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π API /api/generate
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"
            
            payload = {
                "model": self.model,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": temperature or self.temperature
                }
            }
            
            if max_tokens:
                payload["options"]["num_predict"] = max_tokens
            
            logger.debug(f"üì§ Ollama –∑–∞–ø—Ä–æ—Å –∫ /api/generate: model={self.model}, timeout={request_timeout}s")
            response = await self.client.post("/api/generate", json=payload, timeout=request_timeout)
            response.raise_for_status()
            
            result = response.json()
            content = result.get("response", "")
            logger.debug(f"‚úÖ Ollama –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ /api/generate ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return content
            
        except httpx.TimeoutException as e:
            logger.error(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama (timeout={request_timeout}s): {e}")
            raise ConnectionError(f"Ollama –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –≤ —Ç–µ—á–µ–Ω–∏–µ {request_timeout} —Å–µ–∫—É–Ω–¥. –ú–æ–¥–µ–ª—å {self.model} –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ–π –∏–ª–∏ —Å–µ—Ä–≤–µ—Ä –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω.")
        except httpx.HTTPStatusError as e:
            logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ Ollama: {e.response.status_code} - {e.response.text[:200]}")
            raise
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Ollama: {e}", exc_info=True)
            raise
    
    async def _generate_openai(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI/ProxyAPI"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        request_timeout = timeout if timeout is not None else self.timeout
        try:
            messages = []
            
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            
            messages.append({"role": "user", "content": prompt})
            
            logger.debug(f"üì§ OpenAI –∑–∞–ø—Ä–æ—Å: model={self.model}, timeout={request_timeout}s, prompt_len={len(prompt)}")
            
            # –ü–µ—Ä–µ–¥–∞–µ–º timeout –≤ –º–µ—Ç–æ–¥ create() –∫–∞–∫ –≤ sql4A
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens or 2000,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                timeout=request_timeout  # –Ø–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–µ–º timeout –≤ –∑–∞–ø—Ä–æ—Å
            )
            
            content = response.choices[0].message.content
            logger.debug(f"‚úÖ OpenAI –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return content
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ OpenAI: {e}")
            raise
    
    async def _generate_gemini(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Gemini/ProxyAPI —á–µ—Ä–µ–∑ REST API"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        request_timeout = timeout if timeout is not None else self.timeout
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∑–∞–ø—Ä–æ—Å–∞
            parts = [{"text": prompt}]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ ProxyAPI
            # https://api.proxyapi.ru/google/v1beta/models/{model}:generateContent
            request_data = {
                "contents": [{
                    "parts": parts
                }],
                "generationConfig": {
                    "temperature": temperature or self.temperature,
                    "maxOutputTokens": max_tokens or 8000,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è Gemini 3 Pro
                }
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º system instruction –µ—Å–ª–∏ –µ—Å—Ç—å
            if system_prompt:
                request_data["systemInstruction"] = {
                    "parts": [{"text": system_prompt}]
                }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è ProxyAPI
            # –§–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–∏: gemini-3-pro-preview -> models/gemini-3-pro-preview:generateContent
            model_endpoint = f"/v1beta/models/{self.model}:generateContent"
            
            logger.debug(f"üì§ Gemini –∑–∞–ø—Ä–æ—Å –∫ ProxyAPI: {self.base_url}{model_endpoint}, timeout={request_timeout}s")
            
            response = await self.client.post(
                model_endpoint,
                json=request_data,
                timeout=request_timeout
            )
            response.raise_for_status()
            
            result = response.json()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            # –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {"candidates": [{"content": {"parts": [{"text": "..."}]}}]}
            candidates = result.get("candidates", [])
            if candidates:
                candidate = candidates[0]
                finish_reason = candidate.get("finishReason", "")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏—á–∏–Ω—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                if finish_reason == "MAX_TOKENS":
                    logger.warning(f"‚ö†Ô∏è Gemini –¥–æ—Å—Ç–∏–≥ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ (finishReason: {finish_reason})")
                elif finish_reason == "SAFETY":
                    logger.warning(f"‚ö†Ô∏è Gemini –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –ø–æ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (finishReason: {finish_reason})")
                    raise Exception(f"Gemini –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –ø–æ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å.")
                elif finish_reason == "RECITATION":
                    logger.warning(f"‚ö†Ô∏è Gemini –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –∏–∑-–∑–∞ —Ä–µ—Ü–∏—Ç–∞—Ü–∏–∏ (finishReason: {finish_reason})")
                    raise Exception(f"Gemini –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –∏–∑-–∑–∞ —Ä–µ—Ü–∏—Ç–∞—Ü–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å.")
                
                content = candidate.get("content", {})
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ content –∏ –Ω–µ –ø—É—Å—Ç–æ–π –ª–∏ –æ–Ω
                if not content:
                    # –ü—É—Å—Ç–æ–π content –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å, —á—Ç–æ Gemini –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª thinking tokens, –Ω–æ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —Ç–µ–∫—Å—Ç
                    usage_metadata = result.get("usageMetadata", {})
                    thoughts_token_count = usage_metadata.get("thoughtsTokenCount", 0)
                    
                    if thoughts_token_count > 0:
                        logger.warning(f"‚ö†Ô∏è Gemini –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª thinking tokens ({thoughts_token_count}), –Ω–æ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç (finishReason: {finish_reason})")
                        raise Exception(
                            f"Gemini –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª thinking tokens, –Ω–æ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç. "
                            f"–í–æ–∑–º–æ–∂–Ω–æ, –º–æ–¥–µ–ª—å —Ä–µ—à–∏–ª–∞, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. "
                            f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å."
                        )
                    else:
                        logger.warning(f"‚ö†Ô∏è Gemini –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π content (finishReason: {finish_reason})")
                        raise Exception(
                            f"Gemini –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç (finishReason: {finish_reason}). "
                            f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å maxOutputTokens."
                        )
                
                parts = content.get("parts", [])
                if parts:
                    text = parts[0].get("text", "")
                    if text:
                        logger.debug(f"‚úÖ Gemini –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        return text
                    else:
                        logger.warning(f"‚ö†Ô∏è Gemini –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –≤ parts (finishReason: {finish_reason})")
                        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π, –Ω–æ –µ—Å—Ç—å finishReason, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
                        if finish_reason:
                            raise Exception(f"Gemini –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç (finishReason: {finish_reason}). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å maxOutputTokens –∏–ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å.")
                else:
                    logger.warning(f"‚ö†Ô∏è Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª parts –≤ content (finishReason: {finish_reason}, content: {content})")
                    raise Exception(
                        f"Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª parts –≤ content (finishReason: {finish_reason}). "
                        f"–í–æ–∑–º–æ–∂–Ω–æ, –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ thinking tokens –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. "
                        f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."
                    )
            
            # –ï—Å–ª–∏ –Ω–µ—Ç candidates –∏–ª–∏ –æ–Ω–∏ –ø—É—Å—Ç—ã–µ
            usage_metadata = result.get("usageMetadata", {})
            error_msg = f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Gemini. "
            if usage_metadata:
                error_msg += f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {usage_metadata.get('totalTokenCount', 0)}. "
            error_msg += f"–û—Ç–≤–µ—Ç: {result}"
            raise Exception(error_msg)
            
        except httpx.HTTPError as e:
            logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ Gemini: {e.response.status_code if hasattr(e, 'response') else 'unknown'} - {e.response.text[:200] if hasattr(e, 'response') else str(e)}")
            raise
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Gemini: {e}")
            raise
    
    async def generate_json(
        self,
        prompt: str,
        system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è JSON –æ—Ç–≤–µ—Ç–∞
        
        Args:
            prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç
        """
        import json
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è JSON —Ñ–æ—Ä–º–∞—Ç–∞
        json_prompt = f"{prompt}\n\n–í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."
        
        response = await self.generate(json_prompt, system_prompt)
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                return json.loads(response)
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            logger.error(f"–û—Ç–≤–µ—Ç: {response}")
            raise


# Singleton instance
_llm_client_instance: Optional[LLMClient] = None


def get_llm_client(provider: Optional[str] = None) -> LLMClient:
    """
    –ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä LLM –∫–ª–∏–µ–Ω—Ç–∞ (singleton)
    
    Args:
        provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ, —Å–∏–Ω–≥–ª—Ç–æ–Ω –±—É–¥–µ—Ç –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
    """
    global _llm_client_instance
    
    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏ –æ–Ω –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ, –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º
    if provider and (_llm_client_instance is None or _llm_client_instance.provider != provider.lower()):
        _llm_client_instance = None
    
    if _llm_client_instance is None:
        _llm_client_instance = LLMClient(provider=provider)
    
    return _llm_client_instance


def reset_llm_client():
    """–°–±—Ä–æ—Å–∏—Ç—å —Å–∏–Ω–≥–ª—Ç–æ–Ω LLM –∫–ª–∏–µ–Ω—Ç–∞ (–¥–ª—è –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –Ω–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏)"""
    global _llm_client_instance
    _llm_client_instance = None

