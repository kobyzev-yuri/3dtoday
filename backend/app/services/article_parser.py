"""
–ü–∞—Ä—Å–µ—Ä —Å—Ç–∞—Ç–µ–π —Å 3dtoday.ru
–°–∫–∞—á–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç—å—é –ø–æ URL –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
"""

import os
import logging
import re
from typing import Dict, Any, List, Optional
from pathlib import Path
from urllib.parse import urljoin, urlparse
import httpx
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
load_dotenv(dotenv_path=Path(__file__).resolve().parents[3] / "config.env")

logger = logging.getLogger(__name__)


class ArticleParser:
    """
    –ü–∞—Ä—Å–µ—Ä —Å—Ç–∞—Ç–µ–π —Å —Å–∞–π—Ç–∞ 3dtoday.ru
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        self.base_url = "https://3dtoday.ru"
        self.timeout = float(os.getenv("ARTICLE_PARSER_TIMEOUT", os.getenv("PARSER_TIMEOUT", "30")))
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    async def parse_article(self, url: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏ –ø–æ URL
        
        Args:
            url: URL —Å—Ç–∞—Ç—å–∏ –Ω–∞ 3dtoday.ru
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ URL
            if not url.startswith("http"):
                url = urljoin(self.base_url, url)
            
            logger.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏: {url}")
            
            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ HTML
            async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                response = await client.get(url)
                response.raise_for_status()
                html = response.text
            
            # –ü–∞—Ä—Å–∏–Ω–≥ HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            article_data = {
                "url": url,
                "title": self._extract_title(soup),
                "content": self._extract_content(soup),
                "section": self._extract_section(soup, url),
                "date": self._extract_date(soup),
                "images": self._extract_images(soup, url),
                "author": self._extract_author(soup),
                "tags": self._extract_tags(soup)
            }
            
            logger.info(f"‚úÖ –°—Ç–∞—Ç—å—è —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–∞: {article_data['title']}")
            return article_data
            
        except httpx.HTTPError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ HTTP –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ {url}: {e}", exc_info=True)
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        selectors = [
            'h1.article-title',
            'h1',
            '.article-header h1',
            'title'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if title and len(title) > 5:
                    return title
        
        return "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç–∞—Ç—å–∏"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç–µ–π 3dtoday.ru
        selectors = [
            '.blog_post_body',  # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç–µ–π 3dtoday.ru
            '.article-content',
            '.article-text',
            '.post-content',
            '.blog-post-content',
            '.entry-content',
            'article .content',
            'article',
            '.content',
            'main article',
            'main .content',
            '[class*="post"]',
            '[class*="article"]',
            'main div[class*="post"]',
            'main div[class*="content"]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏, –Ω–∞–≤–∏–≥–∞—Ü–∏—é, —Ä–µ–∫–ª–∞–º—É
                for unwanted in element(["script", "style", "nav", "footer", "aside", "header", 
                                         ".sidebar", ".menu", ".navigation", ".breadcrumbs",
                                         ".advertisement", ".ads", "[class*='ad']"]):
                    unwanted.decompose()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                content = element.get_text(separator='\n', strip=True)
                if content and len(content) > 100:
                    # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
                    content = re.sub(r'\n{3,}', '\n\n', content)
                    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
                    lines = content.split('\n')
                    filtered_lines = []
                    skip_keywords = ['–±–ª–æ–≥–∏', '3d-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ', '3d-–ø–µ—á–∞—Ç—å', 'reprap', 
                                   '–∞–∫—Ü–∏–∏', '–±–∏–∑–Ω–µ—Å', '–Ω–æ–≤–æ—Å—Ç–∏', '–æ–±–∑–æ—Ä—ã', '–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ',
                                   '—Ä–∞–∑–Ω–æ–µ', '—Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã', '—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ', '—Ç–µ—Ö–Ω–∏—á–∫–∞',
                                   '3d-–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ', '3d-–ø—Ä–∏–Ω—Ç–µ—Ä—ã', '3d-—Å–∫–∞–Ω–µ—Ä—ã', '3d-–º–æ–¥–µ–ª–∏',
                                   '–≤–æ–π—Ç–∏', '–Ω–æ–≤–æ—Å—Ç–∏', '–ø–æ–ø—É–ª—è—Ä–Ω–æ–µ', '–∞–∫—Ü–∏–∏', '–æ–±—ä—è–≤–ª–µ–Ω–∏—è',
                                   '–≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã', '–º—ã –ø–µ—á–∞—Ç–∞–µ–º']
                    
                    for line in lines:
                        line_lower = line.lower().strip()
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –Ω–∞–≤–∏–≥–∞—Ü–∏–µ–π
                        if any(keyword in line_lower for keyword in skip_keywords) and len(line.strip()) < 50:
                            continue
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ (–≤–µ—Ä–æ—è—Ç–Ω–æ –Ω–∞–≤–∏–≥–∞—Ü–∏—è)
                        if len(line.strip()) < 3:
                            continue
                        filtered_lines.append(line)
                    
                    content = '\n'.join(filtered_lines)
                    content = re.sub(r'\n{3,}', '\n\n', content)
                    
                    if len(content) > 100:
                        return content
        
        # Fallback: –∏—â–µ–º –≤ main –≤—Å–µ div —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–∞
        main = soup.find('main')
        if main:
            # –£–¥–∞–ª—è–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏—é –∏–∑ main
            for unwanted in main(["script", "style", "nav", "footer", "aside", "header",
                                 ".sidebar", ".menu", ".navigation", ".breadcrumbs"]):
                unwanted.decompose()
            
            # –ò—â–µ–º —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫ –≤ main
            max_text = ""
            max_len = 0
            
            for div in main.find_all('div', recursive=True):
                text = div.get_text(separator='\n', strip=True)
                # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏—é
                if len(text) > max_len and len(text) > 200:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –Ω–∞–≤–∏–≥–∞—Ü–∏—è
                    text_lower = text.lower()
                    nav_indicators = ['–±–ª–æ–≥–∏', '–≤–æ–π—Ç–∏', '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è', '–æ—Ç–ø–∏—Å–∞—Ç—å—Å—è', 
                                    '—Ä–µ–∫–ª–∞–º–∞', '—Ä–µ–∫–ª–∞–º–Ω–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ']
                    if not any(indicator in text_lower[:200] for indicator in nav_indicators):
                        max_text = text
                        max_len = len(text)
            
            if max_text:
                content = re.sub(r'\n{3,}', '\n\n', max_text)
                if len(content) > 100:
                    return content
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –≤–µ—Å—å main
            content = main.get_text(separator='\n', strip=True)
            content = re.sub(r'\n{3,}', '\n\n', content)
            if len(content) > 100:
                return content
        
        return ""
    
    def _extract_section(self, soup: BeautifulSoup, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç–∞—Ç—å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã 3dtoday.ru"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–∞
        url_lower = url.lower()
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è wiki —Å—Ç—Ä–∞–Ω–∏—Ü
        if "/wiki/" in url_lower:
            # –≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–∑ –≤–∏–∫–∏–ø–µ–¥–∏–∏ 3D-–ø–µ—á–∞—Ç–∏
            if "3dprinter" in url_lower or "–ø—Ä–∏–Ω—Ç–µ—Ä" in url_lower:
                return "3D-–ø–µ—á–∞—Ç—å"  # –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è
            elif "material" in url_lower or "–º–∞—Ç–µ—Ä–∏–∞–ª" in url_lower or "filament" in url_lower:
                return "–†–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã"
            elif "equipment" in url_lower or "–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ" in url_lower:
                return "–û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ"
            elif "problem" in url_lower or "–ø—Ä–æ–±–ª–µ–º" in url_lower or "issue" in url_lower:
                return "–¢–µ—Ö–Ω–∏—á–∫–∞"
            else:
                return "3D-–ø–µ—á–∞—Ç—å"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –≤–∏–∫–∏
        
        # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ breadcrumbs –∏–ª–∏ URL
        breadcrumbs = soup.select('.breadcrumbs a, .breadcrumb a, nav a')
        if breadcrumbs:
            for crumb in breadcrumbs:
                text = crumb.get_text(strip=True)
                # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã KB –Ω–∞ –æ—Å–Ω–æ–≤–µ 3dtoday.ru
                if text in ["–¢–µ—Ö–Ω–∏—á–∫–∞", "3D-–ø–µ—á–∞—Ç—å", "–û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ", "–†–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã", 
                           "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ", "–û–±–∑–æ—Ä—ã", "3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "RepRap"]:
                    return text
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ URL (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ 3dtoday.ru)
        url_lower = url.lower()
        
        if "/we-print/" in url_lower or "/we_print/" in url_lower:
            return "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ"  # "–ú—ã –ø–µ—á–∞—Ç–∞–µ–º" –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é 3D-–ø–µ—á–∞—Ç–∏
        elif "/technical/" in url_lower or "/tech/" in url_lower or "—Ç–µ—Ö–Ω–∏—á–∫–∞" in url_lower:
            return "–¢–µ—Ö–Ω–∏—á–∫–∞"
        elif "/equipment/" in url_lower or "/printer/" in url_lower or "–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ" in url_lower:
            return "–û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ"
        elif "/material/" in url_lower or "–º–∞—Ç–µ—Ä–∏–∞–ª" in url_lower or "—Ä–∞—Å—Ö–æ–¥–Ω—ã–µ" in url_lower:
            return "–†–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã"
        elif "/application/" in url_lower or "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ" in url_lower:
            return "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ"
        elif "/review/" in url_lower or "–æ–±–∑–æ—Ä" in url_lower:
            return "–û–±–∑–æ—Ä—ã"
        elif "/print/" in url_lower or "/printing/" in url_lower or "–ø–µ—á–∞—Ç—å" in url_lower:
            return "3D-–ø–µ—á–∞—Ç—å"
        elif "/blogs/" in url_lower or "/blog/" in url_lower:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–¥–µ–ª –∏–∑ breadcrumbs –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            blog_categories = soup.select('.blog-category, .category, .tag')
            for cat in blog_categories:
                cat_text = cat.get_text(strip=True)
                if cat_text in ["–¢–µ—Ö–Ω–∏—á–∫–∞", "3D-–ø–µ—á–∞—Ç—å", "–û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ", "–†–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã", 
                               "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ", "–û–±–∑–æ—Ä—ã", "3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "RepRap"]:
                    return cat_text
            return "3D-–ø–µ—á–∞—Ç—å"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –±–ª–æ–≥–æ–≤
        elif "/model/" in url_lower or "–º–æ–¥–µ–ª—å" in url_lower:
            return "3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ"
        elif "/reprap/" in url_lower:
            return "RepRap"
        
        return "unknown"
    
    def _extract_date(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        selectors = [
            '.article-date',
            '.post-date',
            'time[datetime]',
            '.date'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                date = element.get('datetime') or element.get_text(strip=True)
                if date:
                    return date
        
        return ""
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        images = []
        
        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ —Å—Ç–∞—Ç—å–∏
        img_tags = soup.select('.article-content img, .article-text img, article img')
        
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if not src:
                continue
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(base_url, src)
            elif not src.startswith('http'):
                src = urljoin(base_url, src)
            
            alt = img.get('alt', '')
            title = img.get('title', '')
            
            images.append({
                "url": src,
                "alt": alt,
                "title": title,
                "description": alt or title or ""
            })
        
        return images
    
    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ —Å—Ç–∞—Ç—å–∏"""
        selectors = [
            '.article-author',
            '.author',
            '.post-author'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        return None
    
    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ —Å—Ç–∞—Ç—å–∏"""
        tags = []
        
        tag_elements = soup.select('.tags a, .tag a, .article-tags a')
        for tag_elem in tag_elements:
            tag = tag_elem.get_text(strip=True)
            if tag:
                tags.append(tag)
        
        return tags

