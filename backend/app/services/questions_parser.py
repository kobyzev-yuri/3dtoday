"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å 3dtoday.ru
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –Ω–∏—Ö
"""

import os
import logging
import re
from typing import Dict, Any, List, Optional
from pathlib import Path
from urllib.parse import urljoin, urlparse
import httpx
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
load_dotenv(dotenv_path=Path(__file__).resolve().parents[3] / "config.env")

logger = logging.getLogger(__name__)


class QuestionsParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å 3dtoday.ru
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        self.base_url = "https://3dtoday.ru"
        self.timeout = float(os.getenv("QUESTIONS_PARSER_TIMEOUT", os.getenv("PARSER_TIMEOUT", "30")))
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    async def parse_question(self, url: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ URL
        
        Args:
            url: URL –≤–æ–ø—Ä–æ—Å–∞ –Ω–∞ 3dtoday.ru/questions/...
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ URL
            if not url.startswith("http"):
                url = urljoin(self.base_url, url)
            
            logger.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞: {url}")
            
            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ HTML
            async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                response = await client.get(url)
                response.raise_for_status()
                html = response.text
            
            # –ü–∞—Ä—Å–∏–Ω–≥ HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞
            question_data = {
                "url": url,
                "title": self._extract_question_title(soup),
                "question": self._extract_question_text(soup),
                "answers": self._extract_answers(soup),
                "author": self._extract_question_author(soup),
                "date": self._extract_question_date(soup),
                "tags": self._extract_question_tags(soup),
                "section": "–í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã"
            }
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–æ–≤
            content_parts = [f"–í–æ–ø—Ä–æ—Å: {question_data['question']}"]
            if question_data.get('answers'):
                content_parts.append("\n–û—Ç–≤–µ—Ç—ã:")
                for i, answer in enumerate(question_data['answers'], 1):
                    content_parts.append(f"\n–û—Ç–≤–µ—Ç {i}: {answer.get('text', '')}")
            
            question_data["content"] = "\n".join(content_parts)
            
            logger.info(f"‚úÖ –í–æ–ø—Ä–æ—Å —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {question_data['title']} ({len(question_data.get('answers', []))} –æ—Ç–≤–µ—Ç–æ–≤)")
            return question_data
            
        except httpx.HTTPError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ HTTP –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–æ–ø—Ä–æ—Å–∞ {url}: {e}", exc_info=True)
            return None
    
    def _extract_question_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤–æ–ø—Ä–æ—Å–∞"""
        selectors = [
            'h1.question-title',
            '.question-header h1',
            'h1',
            'title'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if title and len(title) > 5:
                    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ –∏–∑ title
                    if "3D Today" in title:
                        title = title.split("3D Today")[0].strip()
                    return title
        
        return "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    
    def _extract_question_text(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤–æ–ø—Ä–æ—Å–∞"""
        selectors = [
            '.question-text',
            '.question-content',
            '.question-body',
            'article.question',
            'h1 + p',  # –ü–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            'h1 ~ p',  # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            'main p',  # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ main
            '#question-content',
            '.content p'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                for script in element(["script", "style"]):
                    script.decompose()
                
                text = element.get_text(separator='\n', strip=True)
                if text and len(text) > 20:
                    return re.sub(r'\n{3,}', '\n\n', text)
        
        # Fallback: –∏—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ø–æ—Å–ª–µ h1
        h1 = soup.find('h1')
        if h1:
            # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
            next_p = h1.find_next_sibling('p')
            if next_p:
                text = next_p.get_text(separator='\n', strip=True)
                if text and len(text) > 20:
                    return re.sub(r'\n{3,}', '\n\n', text)
            
            # –ò–ª–∏ –±–µ—Ä–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ø–æ—Å–ª–µ h1 –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            content_parts = []
            for elem in h1.find_next_siblings():
                if elem.name == 'h2' or elem.name == 'h3':
                    break
                if elem.name == 'p':
                    text = elem.get_text(strip=True)
                    if text:
                        content_parts.append(text)
            
            if content_parts:
                return '\n'.join(content_parts)
        
        return ""
    
    def _extract_answers(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        answers = []
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
        answer_elements = soup.select('.answer, .answer-item, .comment-answer, article.answer')
        
        for elem in answer_elements:
            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for script in elem(["script", "style"]):
                script.decompose()
            
            answer_text = elem.get_text(separator='\n', strip=True)
            if answer_text and len(answer_text) > 20:
                answer_author = self._extract_answer_author(elem)
                answer_date = self._extract_answer_date(elem)
                
                answers.append({
                    "text": answer_text,
                    "author": answer_author,
                    "date": answer_date
                })
        
        return answers
    
    def _extract_question_author(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ –≤–æ–ø—Ä–æ—Å–∞"""
        selectors = [
            '.question-author',
            '.author',
            '.question-meta .author'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        return None
    
    def _extract_answer_author(self, answer_elem: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ –æ—Ç–≤–µ—Ç–∞"""
        author_elem = answer_elem.select_one('.answer-author, .author, .comment-author')
        if author_elem:
            return author_elem.get_text(strip=True)
        return None
    
    def _extract_question_date(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –≤–æ–ø—Ä–æ—Å–∞"""
        selectors = [
            '.question-date',
            '.date',
            '.question-meta .date',
            'time[datetime]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                date = element.get('datetime') or element.get_text(strip=True)
                if date:
                    return date
        
        return ""
    
    def _extract_answer_date(self, answer_elem: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –æ—Ç–≤–µ—Ç–∞"""
        date_elem = answer_elem.select_one('.answer-date, .date, time[datetime]')
        if date_elem:
            return date_elem.get('datetime') or date_elem.get_text(strip=True)
        return ""
    
    def _extract_question_tags(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –≤–æ–ø—Ä–æ—Å–∞"""
        tags = []
        
        tag_elements = soup.select('.question-tags a, .tags a, .tag')
        for tag_elem in tag_elements:
            tag = tag_elem.get_text(strip=True)
            if tag:
                tags.append(tag)
        
        return tags

