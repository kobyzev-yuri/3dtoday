"""
–ü–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–ª—è KB
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: HTML, PDF, JSON
"""

import os
import logging
import json
from typing import Dict, Any, List, Optional
from pathlib import Path
from urllib.parse import urlparse
import httpx
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
load_dotenv(dotenv_path=Path(__file__).resolve().parents[3] / "config.env")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∑–∞–ø–∏—Å—å—é –≤ —Ñ–∞–π–ª
try:
    from app.utils.logger_config import get_parser_logger
    logger = get_parser_logger()
except ImportError:
    # Fallback –µ—Å–ª–∏ logger_config –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    logger = logging.getLogger(__name__)
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)


class DocumentParser:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è KB
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç HTML, PDF, JSON
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        self.timeout = float(os.getenv("DOCUMENT_PARSER_TIMEOUT", os.getenv("PARSER_TIMEOUT", "30")))
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    async def parse_document(self, source: str, source_type: Optional[str] = None, max_pages: Optional[int] = None) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        Args:
            source: URL –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∏–ª–∏ JSON —Å—Ç—Ä–æ–∫–∞
            source_type: –¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (auto, html, pdf, json, url)
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è PDF (None = –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if source_type is None or source_type == "auto":
                source_type = self._detect_source_type(source)
            
            logger.info(f"üì• –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ '{source_type}': {source[:100]}...")
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if source_type == "json":
                return await self._parse_json(source)
            elif source_type == "pdf":
                return await self._parse_pdf(source, max_pages=max_pages)
            elif source_type == "html" or source_type == "url":
                return await self._parse_html(source)
            else:
                logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}", exc_info=True)
            return None
    
    def _detect_source_type(self, source: str) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ JSON —Å—Ç—Ä–æ–∫—É
        if source.strip().startswith('{') or source.strip().startswith('['):
            try:
                json.loads(source)
                return "json"
            except:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ URL
        parsed = urlparse(source)
        if parsed.scheme in ('http', 'https'):
            if source.lower().endswith('.pdf'):
                return "pdf"
            else:
                return "html"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        if os.path.exists(source):
            if source.lower().endswith('.pdf'):
                return "pdf"
            elif source.lower().endswith('.json'):
                return "json"
            elif source.lower().endswith(('.html', '.htm')):
                return "html"
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º HTML/URL
        return "html"
    
    async def _parse_json(self, source: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ JSON –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ï—Å–ª–∏ —ç—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            if os.path.exists(source):
                with open(source, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                # –ï—Å–ª–∏ —ç—Ç–æ JSON —Å—Ç—Ä–æ–∫–∞
                data = json.loads(source)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ KB
            if not isinstance(data, dict):
                logger.error("‚ùå JSON –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º")
                return None
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ KB
            article_data = {
                "title": data.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                "content": data.get("content", data.get("text", "")),
                "url": data.get("url", ""),
                "section": data.get("section", data.get("category", "unknown")),
                "date": data.get("date", ""),
                "author": data.get("author"),
                "tags": data.get("tags", []),
                "images": data.get("images", []),
                "content_type": data.get("content_type", "article"),  # article, documentation, comparison, technical
                "problem_type": data.get("problem_type"),
                "printer_models": data.get("printer_models", []),
                "materials": data.get("materials", []),
                "symptoms": data.get("symptoms", []),
                "solutions": data.get("solutions", []),
                "metadata": data.get("metadata", {})
            }
            
            logger.info(f"‚úÖ JSON –¥–æ–∫—É–º–µ–Ω—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {article_data['title']}")
            return article_data
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ JSON: {e}")
            return None
    
    async def _parse_pdf(self, source: str, max_pages: Optional[int] = None) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            source: URL –∏–ª–∏ –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (None = –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è PDF
            try:
                import PyPDF2
            except ImportError:
                logger.error("‚ùå PyPDF2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install PyPDF2")
                return None
            
            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF –µ—Å–ª–∏ —ç—Ç–æ URL
            pdf_content = None
            if source.startswith('http'):
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                    response = await client.get(source)
                    response.raise_for_status()
                    pdf_content = response.content
            else:
                # –ß—Ç–µ–Ω–∏–µ –∏–∑ —Ñ–∞–π–ª–∞
                with open(source, 'rb') as f:
                    pdf_content = f.read()
            
            if not pdf_content:
                return None
            
            # –ü–∞—Ä—Å–∏–Ω–≥ PDF
            import io
            pdf_file = io.BytesIO(pdf_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            total_pages = len(pdf_reader.pages)
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
            pages_to_parse = total_pages
            if max_pages is not None and max_pages > 0:
                pages_to_parse = min(max_pages, total_pages)
                if pages_to_parse < total_pages:
                    logger.info(f"üìÑ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF: {pages_to_parse} –∏–∑ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            content_parts = []
            images = []
            import base64
            import tempfile
            import os
            import hashlib
            
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyMuPDF –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            use_pymupdf = False
            try:
                import fitz  # PyMuPDF
                use_pymupdf = True
                logger.info("üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PyMuPDF –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ PDF")
            except ImportError:
                logger.info("‚ÑπÔ∏è PyMuPDF –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PyPDF2 (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–º)")
            
            # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º PyMuPDF, –æ—Ç–∫—Ä—ã–≤–∞–µ–º PDF —á–µ—Ä–µ–∑ –Ω–µ–≥–æ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            pdf_images_pymupdf = []
            if use_pymupdf:
                try:
                    pdf_doc_fitz = fitz.open(source if not source.startswith('http') else None, stream=pdf_content if source.startswith('http') else None, filetype="pdf")
                    for page_num_fitz in range(min(pages_to_parse, len(pdf_doc_fitz))):
                        page_fitz = pdf_doc_fitz[page_num_fitz]
                        image_list = page_fitz.get_images()
                        for img_index, img in enumerate(image_list):
                            try:
                                xref = img[0]
                                base_image = pdf_doc_fitz.extract_image(xref)
                                image_bytes = base_image["image"]
                                image_ext = base_image["ext"]
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                                image_hash = hashlib.md5(image_bytes).hexdigest()[:8]
                                temp_dir = Path(tempfile.gettempdir()) / "pdf_images"
                                temp_dir.mkdir(exist_ok=True)
                                temp_image_path = temp_dir / f"pdf_page_{page_num_fitz + 1}_img_{img_index + 1}_{image_hash}.{image_ext}"
                                
                                with open(temp_image_path, 'wb') as img_file:
                                    img_file.write(image_bytes)
                                
                                # –°–æ–∑–¥–∞–µ–º base64 –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —á–µ—Ä–µ–∑ API
                                image_base64 = base64.b64encode(image_bytes).decode('utf-8')
                                
                                pdf_images_pymupdf.append({
                                    "url": str(temp_image_path),
                                    "alt": f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num_fitz + 1}",
                                    "title": f"Image {img_index + 1}",
                                    "description": f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_index + 1} —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num_fitz + 1} PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                                    "data": image_base64,
                                    "mime_type": f"image/{image_ext}",
                                    "page": page_num_fitz + 1,
                                    "image_index": img_index + 1,
                                    "size_bytes": len(image_bytes),
                                    "temp_path": str(temp_image_path)  # –ü—É—Ç—å –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                                })
                                
                                logger.info(f"üì∑ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ PyMuPDF: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num_fitz + 1}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_index + 1}, —Ä–∞–∑–º–µ—Ä {len(image_bytes)} –±–∞–π—Ç, —Ñ–æ—Ä–º–∞—Ç {image_ext}")
                            except Exception as img_error:
                                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_index + 1} —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num_fitz + 1} —á–µ—Ä–µ–∑ PyMuPDF: {img_error}")
                    pdf_doc_fitz.close()
                except Exception as pymupdf_error:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ PyMuPDF: {pymupdf_error}, –∏—Å–ø–æ–ª—å–∑—É–µ–º PyPDF2")
                    use_pymupdf = False
            
            for page_num in range(pages_to_parse):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                if text:
                    content_parts.append(text)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                try:
                    # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ page.images
                    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: PyPDF2 –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —Ç–∏–ø–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                    if hasattr(page, 'images'):
                        try:
                            page_images = page.images
                            if page_images:
                                for img_num, image_file_object in enumerate(page_images):
                                    try:
                                        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                        image_data = image_file_object.data
                                        
                                        if not image_data or len(image_data) == 0:
                                            continue
                                        
                                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
                                        ext = 'jpg'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                                        if hasattr(image_file_object, 'name') and image_file_object.name:
                                            name_ext = image_file_object.name.split('.')[-1].lower()
                                            if name_ext in ['jpg', 'jpeg', 'png', 'gif', 'bmp']:
                                                ext = name_ext
                                        
                                        # –°–æ–∑–¥–∞–µ–º base64 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —á–µ—Ä–µ–∑ API
                                        image_base64 = base64.b64encode(image_data).decode('utf-8')
                                        
                                        images.append({
                                            "url": f"pdf_image_page_{page_num + 1}_img_{img_num + 1}.{ext}",
                                            "alt": f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}",
                                            "title": image_file_object.name if hasattr(image_file_object, 'name') and image_file_object.name else f"Image {img_num + 1}",
                                            "description": f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_num + 1} —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1} PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                                            "data": image_base64,  # Base64 –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                            "mime_type": f"image/{ext}",
                                            "page": page_num + 1,
                                            "image_index": img_num + 1,
                                            "size_bytes": len(image_data)
                                        })
                                        
                                        logger.debug(f"üì∑ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_num + 1}, —Ä–∞–∑–º–µ—Ä {len(image_data)} –±–∞–π—Ç")
                                    except Exception as img_error:
                                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_num + 1} —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {img_error}")
                        except Exception as images_error:
                            # PyPDF2 –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —Ç–∏–ø–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, PA mode)
                            logger.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {images_error}")
                            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ /XObject
                            try:
                                if '/XObject' in page.get('/Resources', {}):
                                    xobjects = page['/Resources']['/XObject'].get_object()
                                    img_count = 0
                                    for obj_name in xobjects:
                                        obj = xobjects[obj_name]
                                        if obj.get('/Subtype') == '/Image':
                                            img_count += 1
                                    if img_count > 0:
                                        logger.info(f"‚ÑπÔ∏è –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num + 1} –Ω–∞–π–¥–µ–Ω–æ {img_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–æ PyPDF2 –Ω–µ –º–æ–∂–µ—Ç –∏—Ö –∏–∑–≤–ª–µ—á—å (–∏–∑–≤–µ—Å—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏)")
                            except Exception:
                                pass
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num + 1}: {e}")
            
            content = "\n\n".join(content_parts)
            
            if pages_to_parse < total_pages:
                content += f"\n\n[–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {pages_to_parse}]"
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ PyMuPDF, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã
            if pdf_images_pymupdf:
                images = pdf_images_pymupdf
                logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ PDF —á–µ—Ä–µ–∑ PyMuPDF: {len(images)}")
            elif images:
                logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ PDF —á–µ—Ä–µ–∑ PyPDF2: {len(images)}")
            else:
                logger.info("‚ÑπÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ PDF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞)
            if images and content:
                # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω 3D-–ø–µ—á–∞—Ç–∏, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–∂–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã
                # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∞–≥–µ–Ω—Ç–æ–º-–±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä–µ–º
                relevant_keywords = ['3d', '–ø—Ä–∏–Ω—Ç–µ—Ä', '–ø–µ—á–∞—Ç—å', 'filament', 'pla', 'petg', 'abs', 'printer', 'extruder', 'bed', 'nozzle', 'layer', 'stringing', 'warping']
                content_lower = content.lower()
                is_3d_printing_related = any(keyword in content_lower for keyword in relevant_keywords)
                
                if is_3d_printing_related:
                    logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω 3D-–ø–µ—á–∞—Ç–∏, –≤—Å–µ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å—á–∏—Ç–∞—é—Ç—Å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏")
                else:
                    logger.info(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω 3D-–ø–µ—á–∞—Ç–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ IndirectObject)
            metadata = {}
            if pdf_reader.metadata:
                try:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –æ–±—ã—á–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è IndirectObject
                    for key, value in pdf_reader.metadata.items():
                        try:
                            # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ - IndirectObject, –ø–æ–ª—É—á–∞–µ–º –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–µ
                            if hasattr(value, 'get_object'):
                                metadata[key] = str(value.get_object())
                            else:
                                metadata[key] = str(value) if value is not None else ""
                        except Exception:
                            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
                            metadata[key] = str(value) if value is not None else ""
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö PDF: {e}")
                    metadata = {}
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            def safe_get_metadata(key, default=""):
                try:
                    value = metadata.get(key, default)
                    if isinstance(value, str):
                        return value
                    return str(value) if value is not None else default
                except Exception:
                    return default
            
            title = safe_get_metadata("/Title", "")
            if not title:
                title = Path(source).stem if not source.startswith('http') else "PDF Document"
            
            article_data = {
                "title": title,
                "content": content,
                "url": source if source.startswith('http') else "",
                "section": "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è",
                "date": safe_get_metadata("/CreationDate", ""),
                "author": safe_get_metadata("/Author"),
                "tags": [],
                "images": images,  # –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ PDF
                "content_type": "documentation",  # PDF –æ–±—ã—á–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
                "metadata": {
                    "pages": total_pages,
                    "pages_parsed": pages_to_parse,
                    "images_count": len(images),
                    "pdf_metadata": metadata
                }
            }
            
            logger.info(f"‚úÖ PDF –¥–æ–∫—É–º–µ–Ω—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {article_data['title']} ({pages_to_parse} –∏–∑ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü)")
            return article_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF: {e}", exc_info=True)
            return None
    
    async def _parse_html(self, source: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç ArticleParser)"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            page_type = await self._detect_page_type(source)
            
            if page_type == "questions_list":
                # –≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤ - –Ω–µ –ø–∞—Ä—Å–∏–º –∫–∞–∫ —Å—Ç–∞—Ç—å—é
                logger.warning(f"‚ö†Ô∏è URL —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤, –∞ –Ω–µ —Å—Ç–∞—Ç—å–µ–π: {source}")
                return {
                    "title": "–°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤",
                    "content": "–≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤. –î–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ KB –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ URL –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.",
                    "url": source,
                    "section": "–í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã",
                    "date": "",
                    "images": [],
                    "content_type": "questions_list",
                    "is_questions_list": True,
                    "error": "–≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤, –∞ –Ω–µ –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ URL –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞."
                }
            
            # –ò–º–ø–æ—Ä—Ç –ø–∞—Ä—Å–µ—Ä–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø—É—Ç–µ–º
            import sys
            from pathlib import Path
            sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
            
            # –ï—Å–ª–∏ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É–µ–º QuestionsParser
            if page_type == "question":
                try:
                    from backend.app.services.questions_parser import QuestionsParser
                except ImportError:
                    from app.services.questions_parser import QuestionsParser
                
                parser = QuestionsParser()
                question_data = await parser.parse_question(source)
                
                if question_data:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç —Å—Ç–∞—Ç—å–∏
                    article_data = {
                        "title": question_data.get("title", ""),
                        "content": question_data.get("content", ""),
                        "url": question_data.get("url", ""),
                        "section": question_data.get("section", "–í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã"),
                        "date": question_data.get("date", ""),
                        "author": question_data.get("author"),
                        "tags": question_data.get("tags", []),
                        "images": [],
                        "content_type": "article",  # –í–æ–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Å—Ç–∞—Ç—å–∏
                        "question_data": question_data  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    }
                    return article_data
                return None
            
            # –î–ª—è —Å—Ç–∞—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º ArticleParser
            try:
                from backend.app.services.article_parser import ArticleParser
            except ImportError:
                from app.services.article_parser import ArticleParser
            
            parser = ArticleParser()
            article_data = await parser.parse_article(source)
            
            if article_data:
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                content_type = self._detect_content_type(article_data)
                article_data["content_type"] = content_type
            
            return article_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML: {e}", exc_info=True)
            return None
    
    async def _detect_page_type(self, url: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Å—Ç–∞—Ç—å—è, —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤, —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π, –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å)
        
        Returns:
            - "questions_list": —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤ (/questions)
            - "question": –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å (/questions/12345)
            - "blogs_list": —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –±–ª–æ–≥–æ–≤ (/blogs)
            - "article": –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è
        """
        url_lower = url.lower()
        parsed = urlparse(url_lower)
        path = parsed.path.rstrip('/')
        
        # –°—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ (–±–µ–∑ ID –≤ –ø—É—Ç–∏)
        if path == "/questions" or path.endswith("/questions"):
            return "questions_list"
        
        if path == "/blogs" or (path.endswith("/blogs") and not path.endswith("/blogs/")):
            return "blogs_list"
        
        # –û—Ç–¥–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã (–µ—Å—Ç—å ID –≤ –ø—É—Ç–∏)
        if "/questions/" in path:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á–∏—Å–ª–æ–≤–æ–π ID –ø–æ—Å–ª–µ /questions/
            parts = path.split("/questions/")
            if len(parts) > 1 and parts[1]:
                # –ï—Å—Ç—å —á—Ç–æ-—Ç–æ –ø–æ—Å–ª–µ /questions/ - —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å
                return "question"
            else:
                # –ù–µ—Ç ID - —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                return "questions_list"
        
        # –û—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        if "/blogs/" in path or "/blog/" in path:
            return "article"
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç—å–µ–π
        return "article"
    
    def _detect_content_type(self, article_data: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        title_lower = article_data.get("title", "").lower()
        content_lower = article_data.get("content", "").lower()
        section = article_data.get("section", "").lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞
        documentation_keywords = ["–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "manual", "specification"]
        comparison_keywords = ["—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ", "vs", "versus", "—Ä–∞–∑–Ω–∏—Ü–∞", "–æ—Ç–ª–∏—á–∏—è", "comparison"]
        technical_keywords = ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–ø–∞—Ä–∞–º–µ—Ç—Ä—ã", "specs", "technical"]
        problem_keywords = ["–ø—Ä–æ–±–ª–µ–º–∞", "—Ä–µ—à–µ–Ω–∏–µ", "–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "—É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ", "problem", "fix"]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ä–∞–∑–¥–µ–ª—É
        if any(kw in section for kw in documentation_keywords):
            return "documentation"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        text_to_check = title_lower + " " + content_lower[:500]
        
        if any(kw in text_to_check for kw in comparison_keywords):
            return "comparison"
        
        if any(kw in text_to_check for kw in documentation_keywords):
            return "documentation"
        
        if any(kw in text_to_check for kw in technical_keywords):
            return "technical"
        
        if any(kw in text_to_check for kw in problem_keywords):
            return "article"  # –°—Ç–∞—Ç—å—è –æ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return "article"

