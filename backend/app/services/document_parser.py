"""
–ü–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–ª—è KB
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: HTML, PDF, JSON
"""

import os
import logging
import json
from typing import Dict, Any, List, Optional
from pathlib import Path
from urllib.parse import urlparse
import httpx
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
load_dotenv(dotenv_path=Path(__file__).resolve().parents[3] / "config.env")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∑–∞–ø–∏—Å—å—é –≤ —Ñ–∞–π–ª
try:
    from app.utils.logger_config import get_parser_logger
    logger = get_parser_logger()
except ImportError:
    # Fallback –µ—Å–ª–∏ logger_config –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    logger = logging.getLogger(__name__)
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)


class DocumentParser:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è KB
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç HTML, PDF, JSON
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        self.timeout = float(os.getenv("DOCUMENT_PARSER_TIMEOUT", os.getenv("PARSER_TIMEOUT", "30")))
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    async def parse_document(self, source: str, source_type: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        Args:
            source: URL –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∏–ª–∏ JSON —Å—Ç—Ä–æ–∫–∞
            source_type: –¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (auto, html, pdf, json, url)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if source_type is None or source_type == "auto":
                source_type = self._detect_source_type(source)
            
            logger.info(f"üì• –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ '{source_type}': {source[:100]}...")
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if source_type == "json":
                return await self._parse_json(source)
            elif source_type == "pdf":
                return await self._parse_pdf(source)
            elif source_type == "html" or source_type == "url":
                return await self._parse_html(source)
            else:
                logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}", exc_info=True)
            return None
    
    def _detect_source_type(self, source: str) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ JSON —Å—Ç—Ä–æ–∫—É
        if source.strip().startswith('{') or source.strip().startswith('['):
            try:
                json.loads(source)
                return "json"
            except:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ URL
        parsed = urlparse(source)
        if parsed.scheme in ('http', 'https'):
            if source.lower().endswith('.pdf'):
                return "pdf"
            else:
                return "html"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        if os.path.exists(source):
            if source.lower().endswith('.pdf'):
                return "pdf"
            elif source.lower().endswith('.json'):
                return "json"
            elif source.lower().endswith(('.html', '.htm')):
                return "html"
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º HTML/URL
        return "html"
    
    async def _parse_json(self, source: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ JSON –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ï—Å–ª–∏ —ç—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            if os.path.exists(source):
                with open(source, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                # –ï—Å–ª–∏ —ç—Ç–æ JSON —Å—Ç—Ä–æ–∫–∞
                data = json.loads(source)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ KB
            if not isinstance(data, dict):
                logger.error("‚ùå JSON –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º")
                return None
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ KB
            article_data = {
                "title": data.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                "content": data.get("content", data.get("text", "")),
                "url": data.get("url", ""),
                "section": data.get("section", data.get("category", "unknown")),
                "date": data.get("date", ""),
                "author": data.get("author"),
                "tags": data.get("tags", []),
                "images": data.get("images", []),
                "content_type": data.get("content_type", "article"),  # article, documentation, comparison, technical
                "problem_type": data.get("problem_type"),
                "printer_models": data.get("printer_models", []),
                "materials": data.get("materials", []),
                "symptoms": data.get("symptoms", []),
                "solutions": data.get("solutions", []),
                "metadata": data.get("metadata", {})
            }
            
            logger.info(f"‚úÖ JSON –¥–æ–∫—É–º–µ–Ω—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {article_data['title']}")
            return article_data
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ JSON: {e}")
            return None
    
    async def _parse_pdf(self, source: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è PDF
            try:
                import PyPDF2
            except ImportError:
                logger.error("‚ùå PyPDF2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install PyPDF2")
                return None
            
            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF –µ—Å–ª–∏ —ç—Ç–æ URL
            pdf_content = None
            if source.startswith('http'):
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                    response = await client.get(source)
                    response.raise_for_status()
                    pdf_content = response.content
            else:
                # –ß—Ç–µ–Ω–∏–µ –∏–∑ —Ñ–∞–π–ª–∞
                with open(source, 'rb') as f:
                    pdf_content = f.read()
            
            if not pdf_content:
                return None
            
            # –ü–∞—Ä—Å–∏–Ω–≥ PDF
            import io
            pdf_file = io.BytesIO(pdf_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
            content_parts = []
            for page_num, page in enumerate(pdf_reader.pages):
                text = page.extract_text()
                if text:
                    content_parts.append(text)
            
            content = "\n\n".join(content_parts)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            metadata = pdf_reader.metadata or {}
            
            article_data = {
                "title": metadata.get("/Title", Path(source).stem if not source.startswith('http') else "PDF Document"),
                "content": content,
                "url": source if source.startswith('http') else "",
                "section": "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è",
                "date": metadata.get("/CreationDate", ""),
                "author": metadata.get("/Author"),
                "tags": [],
                "images": [],  # PDF –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç—Ä–µ–±—É—é—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                "content_type": "documentation",  # PDF –æ–±—ã—á–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
                "metadata": {
                    "pages": len(pdf_reader.pages),
                    "pdf_metadata": dict(metadata)
                }
            }
            
            logger.info(f"‚úÖ PDF –¥–æ–∫—É–º–µ–Ω—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {article_data['title']} ({len(pdf_reader.pages)} —Å—Ç—Ä–∞–Ω–∏—Ü)")
            return article_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF: {e}", exc_info=True)
            return None
    
    async def _parse_html(self, source: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç ArticleParser)"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            page_type = await self._detect_page_type(source)
            
            if page_type == "questions_list":
                # –≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤ - –Ω–µ –ø–∞—Ä—Å–∏–º –∫–∞–∫ —Å—Ç–∞—Ç—å—é
                logger.warning(f"‚ö†Ô∏è URL —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤, –∞ –Ω–µ —Å—Ç–∞—Ç—å–µ–π: {source}")
                return {
                    "title": "–°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤",
                    "content": "–≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤. –î–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ KB –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ URL –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.",
                    "url": source,
                    "section": "–í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã",
                    "date": "",
                    "images": [],
                    "content_type": "questions_list",
                    "is_questions_list": True,
                    "error": "–≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤, –∞ –Ω–µ –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ URL –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞."
                }
            
            # –ò–º–ø–æ—Ä—Ç –ø–∞—Ä—Å–µ—Ä–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø—É—Ç–µ–º
            import sys
            from pathlib import Path
            sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
            
            # –ï—Å–ª–∏ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É–µ–º QuestionsParser
            if page_type == "question":
                try:
                    from backend.app.services.questions_parser import QuestionsParser
                except ImportError:
                    from app.services.questions_parser import QuestionsParser
                
                parser = QuestionsParser()
                question_data = await parser.parse_question(source)
                
                if question_data:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç —Å—Ç–∞—Ç—å–∏
                    article_data = {
                        "title": question_data.get("title", ""),
                        "content": question_data.get("content", ""),
                        "url": question_data.get("url", ""),
                        "section": question_data.get("section", "–í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã"),
                        "date": question_data.get("date", ""),
                        "author": question_data.get("author"),
                        "tags": question_data.get("tags", []),
                        "images": [],
                        "content_type": "article",  # –í–æ–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Å—Ç–∞—Ç—å–∏
                        "question_data": question_data  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    }
                    return article_data
                return None
            
            # –î–ª—è —Å—Ç–∞—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º ArticleParser
            try:
                from backend.app.services.article_parser import ArticleParser
            except ImportError:
                from app.services.article_parser import ArticleParser
            
            parser = ArticleParser()
            article_data = await parser.parse_article(source)
            
            if article_data:
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                content_type = self._detect_content_type(article_data)
                article_data["content_type"] = content_type
            
            return article_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML: {e}", exc_info=True)
            return None
    
    async def _detect_page_type(self, url: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Å—Ç–∞—Ç—å—è, —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤, —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π, –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å)
        
        Returns:
            - "questions_list": —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤ (/questions)
            - "question": –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å (/questions/12345)
            - "blogs_list": —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –±–ª–æ–≥–æ–≤ (/blogs)
            - "article": –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è
        """
        url_lower = url.lower()
        parsed = urlparse(url_lower)
        path = parsed.path.rstrip('/')
        
        # –°—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ (–±–µ–∑ ID –≤ –ø—É—Ç–∏)
        if path == "/questions" or path.endswith("/questions"):
            return "questions_list"
        
        if path == "/blogs" or (path.endswith("/blogs") and not path.endswith("/blogs/")):
            return "blogs_list"
        
        # –û—Ç–¥–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã (–µ—Å—Ç—å ID –≤ –ø—É—Ç–∏)
        if "/questions/" in path:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á–∏—Å–ª–æ–≤–æ–π ID –ø–æ—Å–ª–µ /questions/
            parts = path.split("/questions/")
            if len(parts) > 1 and parts[1]:
                # –ï—Å—Ç—å —á—Ç–æ-—Ç–æ –ø–æ—Å–ª–µ /questions/ - —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å
                return "question"
            else:
                # –ù–µ—Ç ID - —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                return "questions_list"
        
        # –û—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        if "/blogs/" in path or "/blog/" in path:
            return "article"
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç—å–µ–π
        return "article"
    
    def _detect_content_type(self, article_data: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        title_lower = article_data.get("title", "").lower()
        content_lower = article_data.get("content", "").lower()
        section = article_data.get("section", "").lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞
        documentation_keywords = ["–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "manual", "specification"]
        comparison_keywords = ["—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ", "vs", "versus", "—Ä–∞–∑–Ω–∏—Ü–∞", "–æ—Ç–ª–∏—á–∏—è", "comparison"]
        technical_keywords = ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–ø–∞—Ä–∞–º–µ—Ç—Ä—ã", "specs", "technical"]
        problem_keywords = ["–ø—Ä–æ–±–ª–µ–º–∞", "—Ä–µ—à–µ–Ω–∏–µ", "–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "—É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ", "problem", "fix"]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ä–∞–∑–¥–µ–ª—É
        if any(kw in section for kw in documentation_keywords):
            return "documentation"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        text_to_check = title_lower + " " + content_lower[:500]
        
        if any(kw in text_to_check for kw in comparison_keywords):
            return "comparison"
        
        if any(kw in text_to_check for kw in documentation_keywords):
            return "documentation"
        
        if any(kw in text_to_check for kw in technical_keywords):
            return "technical"
        
        if any(kw in text_to_check for kw in problem_keywords):
            return "article"  # –°—Ç–∞—Ç—å—è –æ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return "article"

