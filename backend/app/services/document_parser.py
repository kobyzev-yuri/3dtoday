"""
–ü–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–ª—è KB
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: HTML, PDF, JSON
"""

import os
import logging
import json
from typing import Dict, Any, List, Optional
from pathlib import Path
from urllib.parse import urlparse
import httpx
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
load_dotenv(dotenv_path=Path(__file__).resolve().parents[3] / "config.env")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∑–∞–ø–∏—Å—å—é –≤ —Ñ–∞–π–ª
try:
    from app.utils.logger_config import get_parser_logger
    logger = get_parser_logger()
except ImportError:
    # Fallback –µ—Å–ª–∏ logger_config –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    logger = logging.getLogger(__name__)
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)


class DocumentParser:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è KB
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç HTML, PDF, JSON
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        self.timeout = float(os.getenv("DOCUMENT_PARSER_TIMEOUT", os.getenv("PARSER_TIMEOUT", "30")))
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    async def parse_document(self, source: str, source_type: Optional[str] = None, max_pages: Optional[int] = None) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        Args:
            source: URL –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∏–ª–∏ JSON —Å—Ç—Ä–æ–∫–∞
            source_type: –¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (auto, html, pdf, json, url)
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è PDF (None = –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if source_type is None or source_type == "auto":
                source_type = self._detect_source_type(source)
            
            logger.info(f"üì• –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ '{source_type}': {source[:100]}...")
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if source_type == "json":
                return await self._parse_json(source)
            elif source_type == "pdf":
                return await self._parse_pdf(source, max_pages=max_pages)
            elif source_type == "html" or source_type == "url":
                return await self._parse_html(source)
            else:
                logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}", exc_info=True)
            return None
    
    def _detect_source_type(self, source: str) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ JSON —Å—Ç—Ä–æ–∫—É
        if source.strip().startswith('{') or source.strip().startswith('['):
            try:
                json.loads(source)
                return "json"
            except:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ URL
        parsed = urlparse(source)
        if parsed.scheme in ('http', 'https'):
            if source.lower().endswith('.pdf'):
                return "pdf"
            else:
                return "html"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        if os.path.exists(source):
            if source.lower().endswith('.pdf'):
                return "pdf"
            elif source.lower().endswith('.json'):
                return "json"
            elif source.lower().endswith(('.html', '.htm')):
                return "html"
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º HTML/URL
        return "html"
    
    async def _parse_json(self, source: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ JSON –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ï—Å–ª–∏ —ç—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            if os.path.exists(source):
                with open(source, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                # –ï—Å–ª–∏ —ç—Ç–æ JSON —Å—Ç—Ä–æ–∫–∞
                data = json.loads(source)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ KB
            if not isinstance(data, dict):
                logger.error("‚ùå JSON –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º")
                return None
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ KB
            article_data = {
                "title": data.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                "content": data.get("content", data.get("text", "")),
                "url": data.get("url", ""),
                "section": data.get("section", data.get("category", "unknown")),
                "date": data.get("date", ""),
                "author": data.get("author"),
                "tags": data.get("tags", []),
                "images": data.get("images", []),
                "content_type": data.get("content_type", "article"),  # article, documentation, comparison, technical
                "problem_type": data.get("problem_type"),
                "printer_models": data.get("printer_models", []),
                "materials": data.get("materials", []),
                "symptoms": data.get("symptoms", []),
                "solutions": data.get("solutions", []),
                "metadata": data.get("metadata", {})
            }
            
            logger.info(f"‚úÖ JSON –¥–æ–∫—É–º–µ–Ω—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {article_data['title']}")
            return article_data
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ JSON: {e}")
            return None
    
    async def _parse_txt(self, source: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ TXT —Ñ–∞–π–ª–∞
        
        Args:
            source: –ü—É—Ç—å –∫ TXT —Ñ–∞–π–ª—É
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            path = Path(source)
            if not path.exists():
                logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {source}")
                return None
            
            logger.info(f"üìÑ –ß—Ç–µ–Ω–∏–µ TXT —Ñ–∞–π–ª–∞: {source}")
            
            # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            try:
                # –ü—Ä–æ–±—É–µ–º UTF-8
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                # –ï—Å–ª–∏ –Ω–µ UTF-8, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                try:
                    with open(path, 'r', encoding='cp1251') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    with open(path, 'r', encoding='latin-1') as f:
                        content = f.read()
            
            if not content or len(content.strip()) < 10:
                logger.warning(f"‚ö†Ô∏è TXT —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {source}")
                return {
                    "title": path.stem,
                    "content": content.strip() if content else "",
                    "url": str(path),
                    "section": "unknown",
                    "date": "",
                    "images": [],
                    "content_type": "article",
                    "error": "–§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π"
                }
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏
            lines = content.strip().split('\n')
            title = ""
            content_start = 0
            
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–æ–±—ã—á–Ω–æ –≤ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ –∏–ª–∏ –ø–æ—Å–ª–µ "–ó–∞–≥–æ–ª–æ–≤–æ–∫:")
            for i, line in enumerate(lines[:5]):
                line = line.strip()
                if line.startswith("–ó–∞–≥–æ–ª–æ–≤–æ–∫:") or line.startswith("Title:"):
                    title = line.split(":", 1)[1].strip()
                    content_start = i + 1
                    break
                elif i == 0 and len(line) > 5 and len(line) < 200:
                    # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                    title = line
                    content_start = 1
                    break
            
            # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
            if not title:
                title = path.stem.replace('_', ' ').replace('-', ' ')
            
            # –û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            if content_start > 0:
                content_text = '\n'.join(lines[content_start:]).strip()
            else:
                content_text = content.strip()
            
            logger.info(f"‚úÖ TXT —Ñ–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(content_text)} —Å–∏–º–≤–æ–ª–æ–≤, –∑–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
            
            return {
                "title": title,
                "content": content_text,
                "url": str(path),
                "section": "unknown",
                "date": "",
                "images": [],
                "content_type": "article"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ TXT: {e}", exc_info=True)
            return None
    
    async def _parse_pdf(self, source: str, max_pages: Optional[int] = None) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            source: URL –∏–ª–∏ –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (None = –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è PDF
            try:
                import PyPDF2
            except ImportError:
                logger.error("‚ùå PyPDF2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install PyPDF2")
                return None
            
            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF –µ—Å–ª–∏ —ç—Ç–æ URL
            pdf_content = None
            if source.startswith('http'):
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                    response = await client.get(source)
                    response.raise_for_status()
                    pdf_content = response.content
            else:
                # –ß—Ç–µ–Ω–∏–µ –∏–∑ —Ñ–∞–π–ª–∞
                with open(source, 'rb') as f:
                    pdf_content = f.read()
            
            if not pdf_content:
                return None
            
            # –ü–∞—Ä—Å–∏–Ω–≥ PDF
            import io
            pdf_file = io.BytesIO(pdf_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            total_pages = len(pdf_reader.pages)
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
            pages_to_parse = total_pages
            if max_pages is not None and max_pages > 0:
                pages_to_parse = min(max_pages, total_pages)
                if pages_to_parse < total_pages:
                    logger.info(f"üìÑ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF: {pages_to_parse} –∏–∑ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            content_parts = []
            images = []
            import base64
            import tempfile
            import os
            import hashlib
            
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyMuPDF –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            use_pymupdf = False
            try:
                import fitz  # PyMuPDF
                use_pymupdf = True
                logger.info("üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PyMuPDF –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ PDF")
            except ImportError:
                logger.info("‚ÑπÔ∏è PyMuPDF –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PyPDF2 (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–º)")
            
            # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º PyMuPDF, –æ—Ç–∫—Ä—ã–≤–∞–µ–º PDF —á–µ—Ä–µ–∑ –Ω–µ–≥–æ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            pdf_images_pymupdf = []
            if use_pymupdf:
                try:
                    pdf_doc_fitz = fitz.open(source if not source.startswith('http') else None, stream=pdf_content if source.startswith('http') else None, filetype="pdf")
                    for page_num_fitz in range(min(pages_to_parse, len(pdf_doc_fitz))):
                        page_fitz = pdf_doc_fitz[page_num_fitz]
                        image_list = page_fitz.get_images()
                        for img_index, img in enumerate(image_list):
                            try:
                                xref = img[0]
                                base_image = pdf_doc_fitz.extract_image(xref)
                                image_bytes = base_image["image"]
                                image_ext = base_image["ext"]
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                                image_hash = hashlib.md5(image_bytes).hexdigest()[:8]
                                temp_dir = Path(tempfile.gettempdir()) / "pdf_images"
                                temp_dir.mkdir(exist_ok=True)
                                temp_image_path = temp_dir / f"pdf_page_{page_num_fitz + 1}_img_{img_index + 1}_{image_hash}.{image_ext}"
                                
                                with open(temp_image_path, 'wb') as img_file:
                                    img_file.write(image_bytes)
                                
                                # –°–æ–∑–¥–∞–µ–º base64 –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —á–µ—Ä–µ–∑ API
                                image_base64 = base64.b64encode(image_bytes).decode('utf-8')
                                
                                pdf_images_pymupdf.append({
                                    "url": str(temp_image_path),
                                    "alt": f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num_fitz + 1}",
                                    "title": f"Image {img_index + 1}",
                                    "description": f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_index + 1} —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num_fitz + 1} PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                                    "data": image_base64,
                                    "mime_type": f"image/{image_ext}",
                                    "page": page_num_fitz + 1,
                                    "image_index": img_index + 1,
                                    "size_bytes": len(image_bytes),
                                    "temp_path": str(temp_image_path)  # –ü—É—Ç—å –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                                })
                                
                                logger.info(f"üì∑ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ PyMuPDF: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num_fitz + 1}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_index + 1}, —Ä–∞–∑–º–µ—Ä {len(image_bytes)} –±–∞–π—Ç, —Ñ–æ—Ä–º–∞—Ç {image_ext}")
                            except Exception as img_error:
                                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_index + 1} —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num_fitz + 1} —á–µ—Ä–µ–∑ PyMuPDF: {img_error}")
                    pdf_doc_fitz.close()
                except Exception as pymupdf_error:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ PyMuPDF: {pymupdf_error}, –∏—Å–ø–æ–ª—å–∑—É–µ–º PyPDF2")
                    use_pymupdf = False
            
            for page_num in range(pages_to_parse):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                if text:
                    content_parts.append(text)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                try:
                    # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ page.images
                    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: PyPDF2 –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —Ç–∏–ø–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                    if hasattr(page, 'images'):
                        try:
                            page_images = page.images
                            if page_images:
                                for img_num, image_file_object in enumerate(page_images):
                                    try:
                                        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                        image_data = image_file_object.data
                                        
                                        if not image_data or len(image_data) == 0:
                                            continue
                                        
                                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
                                        ext = 'jpg'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                                        if hasattr(image_file_object, 'name') and image_file_object.name:
                                            name_ext = image_file_object.name.split('.')[-1].lower()
                                            if name_ext in ['jpg', 'jpeg', 'png', 'gif', 'bmp']:
                                                ext = name_ext
                                        
                                        # –°–æ–∑–¥–∞–µ–º base64 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —á–µ—Ä–µ–∑ API
                                        image_base64 = base64.b64encode(image_data).decode('utf-8')
                                        
                                        images.append({
                                            "url": f"pdf_image_page_{page_num + 1}_img_{img_num + 1}.{ext}",
                                            "alt": f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}",
                                            "title": image_file_object.name if hasattr(image_file_object, 'name') and image_file_object.name else f"Image {img_num + 1}",
                                            "description": f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_num + 1} —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1} PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                                            "data": image_base64,  # Base64 –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                            "mime_type": f"image/{ext}",
                                            "page": page_num + 1,
                                            "image_index": img_num + 1,
                                            "size_bytes": len(image_data)
                                        })
                                        
                                        logger.debug(f"üì∑ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_num + 1}, —Ä–∞–∑–º–µ—Ä {len(image_data)} –±–∞–π—Ç")
                                    except Exception as img_error:
                                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_num + 1} —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {img_error}")
                        except Exception as images_error:
                            # PyPDF2 –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —Ç–∏–ø–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, PA mode)
                            logger.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {images_error}")
                            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ /XObject
                            try:
                                if '/XObject' in page.get('/Resources', {}):
                                    xobjects = page['/Resources']['/XObject'].get_object()
                                    img_count = 0
                                    for obj_name in xobjects:
                                        obj = xobjects[obj_name]
                                        if obj.get('/Subtype') == '/Image':
                                            img_count += 1
                                    if img_count > 0:
                                        logger.info(f"‚ÑπÔ∏è –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num + 1} –Ω–∞–π–¥–µ–Ω–æ {img_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–æ PyPDF2 –Ω–µ –º–æ–∂–µ—Ç –∏—Ö –∏–∑–≤–ª–µ—á—å (–∏–∑–≤–µ—Å—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏)")
                            except Exception:
                                pass
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num + 1}: {e}")
            
            content = "\n\n".join(content_parts)
            
            if pages_to_parse < total_pages:
                content += f"\n\n[–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {pages_to_parse}]"
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ PyMuPDF, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã
            if pdf_images_pymupdf:
                images = pdf_images_pymupdf
                logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ PDF —á–µ—Ä–µ–∑ PyMuPDF: {len(images)}")
            elif images:
                logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ PDF —á–µ—Ä–µ–∑ PyPDF2: {len(images)}")
            else:
                logger.info("‚ÑπÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ PDF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞)
            if images and content:
                # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω 3D-–ø–µ—á–∞—Ç–∏, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–∂–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã
                # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∞–≥–µ–Ω—Ç–æ–º-–±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä–µ–º
                relevant_keywords = ['3d', '–ø—Ä–∏–Ω—Ç–µ—Ä', '–ø–µ—á–∞—Ç—å', 'filament', 'pla', 'petg', 'abs', 'printer', 'extruder', 'bed', 'nozzle', 'layer', 'stringing', 'warping']
                content_lower = content.lower()
                is_3d_printing_related = any(keyword in content_lower for keyword in relevant_keywords)
                
                if is_3d_printing_related:
                    logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω 3D-–ø–µ—á–∞—Ç–∏, –≤—Å–µ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å—á–∏—Ç–∞—é—Ç—Å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏")
                else:
                    logger.info(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω 3D-–ø–µ—á–∞—Ç–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ IndirectObject)
            metadata = {}
            if pdf_reader.metadata:
                try:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –æ–±—ã—á–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è IndirectObject
                    for key, value in pdf_reader.metadata.items():
                        try:
                            # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ - IndirectObject, –ø–æ–ª—É—á–∞–µ–º –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–µ
                            if hasattr(value, 'get_object'):
                                metadata[key] = str(value.get_object())
                            else:
                                metadata[key] = str(value) if value is not None else ""
                        except Exception:
                            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
                            metadata[key] = str(value) if value is not None else ""
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö PDF: {e}")
                    metadata = {}
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            def safe_get_metadata(key, default=""):
                try:
                    value = metadata.get(key, default)
                    if isinstance(value, str):
                        return value
                    return str(value) if value is not None else default
                except Exception:
                    return default
            
            title = safe_get_metadata("/Title", "")
            if not title:
                title = Path(source).stem if not source.startswith('http') else "PDF Document"
            
            article_data = {
                "title": title,
                "content": content,
                "url": source if source.startswith('http') else "",
                "section": "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è",
                "date": safe_get_metadata("/CreationDate", ""),
                "author": safe_get_metadata("/Author"),
                "tags": [],
                "images": images,  # –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ PDF
                "content_type": "documentation",  # PDF –æ–±—ã—á–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
                "metadata": {
                    "pages": total_pages,
                    "pages_parsed": pages_to_parse,
                    "images_count": len(images),
                    "pdf_metadata": metadata
                }
            }
            
            logger.info(f"‚úÖ PDF –¥–æ–∫—É–º–µ–Ω—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {article_data['title']} ({pages_to_parse} –∏–∑ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü)")
            return article_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF: {e}", exc_info=True)
            return None
    
    async def _parse_html(self, source: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ HTML –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π:
        1. Trafilatura (–ª—É—á—à–∏–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)
        2. Readability-lxml (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)
        3. LLM –ø–∞—Ä—Å–∏–Ω–≥ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        4. ArticleParser (–¥–ª—è 3dtoday.ru)
        5. BeautifulSoup fallback
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            page_type = await self._detect_page_type(source)
            
            if page_type == "questions_list":
                # –≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤ - –Ω–µ –ø–∞—Ä—Å–∏–º –∫–∞–∫ —Å—Ç–∞—Ç—å—é
                logger.warning(f"‚ö†Ô∏è URL —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤, –∞ –Ω–µ —Å—Ç–∞—Ç—å–µ–π: {source}")
                return {
                    "title": "–°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤",
                    "content": "–≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤. –î–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ KB –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ URL –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.",
                    "url": source,
                    "section": "–í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã",
                    "date": "",
                    "images": [],
                    "content_type": "questions_list",
                    "is_questions_list": True,
                    "error": "–≠—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤, –∞ –Ω–µ –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ URL –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞."
                }
            
            # –ò–º–ø–æ—Ä—Ç –ø–∞—Ä—Å–µ—Ä–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø—É—Ç–µ–º
            import sys
            from pathlib import Path
            sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
            
            # –ï—Å–ª–∏ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É–µ–º QuestionsParser
            if page_type == "question":
                try:
                    from backend.app.services.questions_parser import QuestionsParser
                except ImportError:
                    from app.services.questions_parser import QuestionsParser
                
                parser = QuestionsParser()
                question_data = await parser.parse_question(source)
                
                if question_data:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç —Å—Ç–∞—Ç—å–∏
                    article_data = {
                        "title": question_data.get("title", ""),
                        "content": question_data.get("content", ""),
                        "url": question_data.get("url", ""),
                        "section": question_data.get("section", "–í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã"),
                        "date": question_data.get("date", ""),
                        "author": question_data.get("author"),
                        "tags": question_data.get("tags", []),
                        "images": [],
                        "content_type": "article",  # –í–æ–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Å—Ç–∞—Ç—å–∏
                        "question_data": question_data  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    }
                    return article_data
                return None
            
            # –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö URL
            
            # –£–†–û–í–ï–ù–¨ 1: Trafilatura (–ª—É—á—à–∏–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)
            logger.info(f"üîç –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Trafilatura...")
            article_data = await self._parse_with_trafilatura(source)
            if article_data and article_data.get("content") and len(article_data.get("content", "")) > 100:
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ —á–µ—Ä–µ–∑ Trafilatura: {len(article_data.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤, {len(article_data.get('images', []))} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                content_type = self._detect_content_type(article_data)
                article_data["content_type"] = content_type
                return article_data
            else:
                logger.info(f"‚ö†Ô∏è Trafilatura –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –º–µ—Ç–æ–¥...")
            
            # –£–†–û–í–ï–ù–¨ 2: Readability-lxml
            logger.info(f"üîç –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Readability...")
            article_data = await self._parse_with_readability(source)
            if article_data and article_data.get("content") and len(article_data.get("content", "")) > 100:
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ —á–µ—Ä–µ–∑ Readability: {len(article_data.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤, {len(article_data.get('images', []))} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                content_type = self._detect_content_type(article_data)
                article_data["content_type"] = content_type
                return article_data
            else:
                logger.info(f"‚ö†Ô∏è Readability –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –º–µ—Ç–æ–¥...")
            
            # –£–†–û–í–ï–ù–¨ 3: ArticleParser (–¥–ª—è 3dtoday.ru –∏ –ø–æ—Ö–æ–∂–∏—Ö —Å–∞–π—Ç–æ–≤)
            try:
                from backend.app.services.article_parser import ArticleParser
            except ImportError:
                from app.services.article_parser import ArticleParser
            
            parser = ArticleParser()
            article_data = await parser.parse_article(source)
            
            if article_data and article_data.get("content") and len(article_data.get("content", "")) > 100:
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ —á–µ—Ä–µ–∑ ArticleParser: {len(article_data.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
                content_type = self._detect_content_type(article_data)
                article_data["content_type"] = content_type
                return article_data
            
            # –£–†–û–í–ï–ù–¨ 4: BeautifulSoup fallback
            article_data = await self._parse_with_beautifulsoup(source)
            if article_data and article_data.get("content") and len(article_data.get("content", "")) > 100:
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ —á–µ—Ä–µ–∑ BeautifulSoup: {len(article_data.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
                content_type = self._detect_content_type(article_data)
                article_data["content_type"] = content_type
                return article_data
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∏ –æ–¥–Ω–∏–º –∏–∑ –º–µ—Ç–æ–¥–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞")
            return {
                "title": "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å",
                "content": "",
                "url": source,
                "section": "unknown",
                "date": "",
                "images": [],
                "content_type": "article",
                "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML: {e}", exc_info=True)
            return None
    
    async def _parse_with_trafilatura(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Trafilatura (–ª—É—á—à–∏–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)"""
        try:
            import trafilatura
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º HTML
            async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                response = await client.get(url)
                response.raise_for_status()
                html = response.text
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Trafilatura
            extracted = trafilatura.extract(
                html,
                include_comments=False,
                include_tables=True,
                include_images=True,
                include_links=True,
                favor_recall=True  # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –ø–æ–ª–Ω–æ—Ç—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            )
            
            if not extracted or len(extracted) < 100:
                return None
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = trafilatura.extract_metadata(html)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = []
            try:
                # Trafilatura –º–æ–∂–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ extract_images
                # –ù–æ —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –∏–∑–≤–ª–µ—á—å –∏—Ö –∏–∑ HTML –Ω–∞–ø—Ä—è–º—É—é
                soup = BeautifulSoup(html, 'html.parser')
                for img in soup.find_all('img'):
                    img_url = img.get('src', '')
                    if img_url:
                        if not img_url.startswith('http'):
                            from urllib.parse import urljoin
                            img_url = urljoin(url, img_url)
                        images.append({
                            "url": img_url,
                            "alt": img.get('alt', ''),
                            "title": img.get('title', '')
                        })
                logger.info(f"üì∑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ HTML")
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            
            return {
                "title": metadata.title if metadata and metadata.title else "",
                "content": extracted,
                "url": url,
                "section": "unknown",
                "date": metadata.date if metadata and metadata.date else "",
                "author": metadata.author if metadata and metadata.author else None,
                "tags": [],
                "images": images,
                "content_type": "article"
            }
            
        except ImportError:
            logger.debug("Trafilatura –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return None
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Trafilatura: {e}")
            return None
    
    async def _parse_with_readability(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Readability-lxml"""
        try:
            from readability import Document
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º HTML
            async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                response = await client.get(url)
                response.raise_for_status()
                html = response.text
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Readability
            doc = Document(html)
            title = doc.title()
            content_html = doc.summary()
            
            if not content_html or len(content_html) < 100:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ HTML
            soup = BeautifulSoup(content_html, 'html.parser')
            content = soup.get_text(separator='\n', strip=True)
            
            if not content or len(content) < 100:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = []
            for img in soup.find_all('img'):
                img_url = img.get('src', '')
                if img_url:
                    if not img_url.startswith('http'):
                        from urllib.parse import urljoin
                        img_url = urljoin(url, img_url)
                    images.append({
                        "url": img_url,
                        "alt": img.get('alt', ''),
                        "title": img.get('title', '')
                    })
            
            return {
                "title": title,
                "content": content,
                "url": url,
                "section": "unknown",
                "date": "",
                "author": None,
                "tags": [],
                "images": images,
                "content_type": "article"
            }
            
        except ImportError:
            logger.debug("Readability-lxml –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return None
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Readability: {e}")
            return None
    
    async def _parse_with_beautifulsoup(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ BeautifulSoup (fallback)"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º HTML
            async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                response = await client.get(url)
                response.raise_for_status()
                html = response.text
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = ""
            title_elem = soup.find('title')
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            if not title:
                h1 = soup.find('h1')
                if h1:
                    title = h1.get_text(strip=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ article, main, –∏–ª–∏ div —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
            content = ""
            content_selectors = [
                'article',
                'main',
                '[role="main"]',
                '.content',
                '.article-content',
                '.post-content',
                '.entry-content',
                '#content',
                '#main-content'
            ]
            
            for selector in content_selectors:
                elem = soup.select_one(selector)
                if elem:
                    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                    for unwanted in elem(["script", "style", "nav", "footer", "aside", "header"]):
                        unwanted.decompose()
                    
                    content = elem.get_text(separator='\n', strip=True)
                    if content and len(content) > 100:
                        break
            
            if not content or len(content) < 100:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = []
            for img in soup.find_all('img'):
                img_url = img.get('src', '')
                if img_url:
                    if not img_url.startswith('http'):
                        from urllib.parse import urljoin
                        img_url = urljoin(url, img_url)
                    images.append({
                        "url": img_url,
                        "alt": img.get('alt', ''),
                        "title": img.get('title', '')
                    })
            
            return {
                "title": title or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è",
                "content": content,
                "url": url,
                "section": "unknown",
                "date": "",
                "author": None,
                "tags": [],
                "images": images,
                "content_type": "article"
            }
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ BeautifulSoup: {e}")
            return None
    
    async def _detect_page_type(self, url: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Å—Ç–∞—Ç—å—è, —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤, —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π, –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å)
        
        Returns:
            - "questions_list": —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤ (/questions)
            - "question": –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å (/questions/12345)
            - "blogs_list": —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –±–ª–æ–≥–æ–≤ (/blogs)
            - "article": –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è
        """
        url_lower = url.lower()
        parsed = urlparse(url_lower)
        path = parsed.path.rstrip('/')
        
        # –°—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ (–±–µ–∑ ID –≤ –ø—É—Ç–∏)
        if path == "/questions" or path.endswith("/questions"):
            return "questions_list"
        
        if path == "/blogs" or (path.endswith("/blogs") and not path.endswith("/blogs/")):
            return "blogs_list"
        
        # –û—Ç–¥–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã (–µ—Å—Ç—å ID –≤ –ø—É—Ç–∏)
        if "/questions/" in path:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á–∏—Å–ª–æ–≤–æ–π ID –ø–æ—Å–ª–µ /questions/
            parts = path.split("/questions/")
            if len(parts) > 1 and parts[1]:
                # –ï—Å—Ç—å —á—Ç–æ-—Ç–æ –ø–æ—Å–ª–µ /questions/ - —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å
                return "question"
            else:
                # –ù–µ—Ç ID - —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                return "questions_list"
        
        # –û—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        if "/blogs/" in path or "/blog/" in path:
            return "article"
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç—å–µ–π
        return "article"
    
    def _detect_content_type(self, article_data: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        title_lower = article_data.get("title", "").lower()
        content_lower = article_data.get("content", "").lower()
        section = article_data.get("section", "").lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞
        documentation_keywords = ["–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "manual", "specification"]
        comparison_keywords = ["—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ", "vs", "versus", "—Ä–∞–∑–Ω–∏—Ü–∞", "–æ—Ç–ª–∏—á–∏—è", "comparison"]
        technical_keywords = ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–ø–∞—Ä–∞–º–µ—Ç—Ä—ã", "specs", "technical"]
        problem_keywords = ["–ø—Ä–æ–±–ª–µ–º–∞", "—Ä–µ—à–µ–Ω–∏–µ", "–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "—É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ", "problem", "fix"]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ä–∞–∑–¥–µ–ª—É
        if any(kw in section for kw in documentation_keywords):
            return "documentation"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        text_to_check = title_lower + " " + content_lower[:500]
        
        if any(kw in text_to_check for kw in comparison_keywords):
            return "comparison"
        
        if any(kw in text_to_check for kw in documentation_keywords):
            return "documentation"
        
        if any(kw in text_to_check for kw in technical_keywords):
            return "technical"
        
        if any(kw in text_to_check for kw in problem_keywords):
            return "article"  # –°—Ç–∞—Ç—å—è –æ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return "article"

